{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import entity_formatter\n",
    "from entity_tagger import entity_tagger as tagger\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MOST IMPORTANT\n",
    "exp_id = \"exp03\" #unique for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm = boto3.client(\"ssm\")\n",
    "s3 = boto3.client(\"s3\")\n",
    "root_url = ssm.get_parameter(Name=f\"/account/root-url\")[\"Parameter\"][\"Value\"]\n",
    "apikey = ssm.get_parameter(Name=\"/account/internal-api-key\")[\"Parameter\"][\"Value\"]\n",
    "v1_url = f\"https://remember.{root_url}\"\n",
    "v2_url = f\"https://rememberv2.{root_url}/latest\"\n",
    "acc_owner = ssm.get_parameter(Name=\"/account/owner\")[\"Parameter\"][\"Value\"].upper()\n",
    "headers = {\"x-api-key\": apikey, \"Authorization\": apikey}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://rememberv2.usbanktraining.heavywater.com/latest'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_this = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rememberv2_query(index={}, filters={}):\n",
    "    url = f\"{v2_url}/query\"\n",
    "    results = {}\n",
    "    try:\n",
    "        payload = {\n",
    "            \"Index\": index,\n",
    "            \"Filter\": filters\n",
    "        }\n",
    "        results = json.loads(requests.post(url=url, data=json.dumps(payload), headers=headers).text)[\"Results\"]\n",
    "    except:\n",
    "        print(traceback.format_exc())    \n",
    "    return results\n",
    "\n",
    "\n",
    "def rememberv2_read(objectid):\n",
    "    url = f\"{v2_url}/read\"\n",
    "    results = {}\n",
    "    try:\n",
    "        payload = {\n",
    "            \"ObjectId\": objectid,\n",
    "        }\n",
    "        results = json.loads(requests.post(url=url, data=json.dumps(payload), headers=headers).text)[\"Results\"]\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def remember_recall(rid, datapoint):\n",
    "    url = f\"{v1_url}/recall?_remember_id={rid}&_datapoint={datapoint}\"\n",
    "    res = {}\n",
    "    try:\n",
    "        res = json.loads(requests.get(url=url).text)[\"datapoints\"][0][\"data\"]\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "    return res\n",
    "    \n",
    "def make_text_blob(word_ocr):\n",
    "    text_list = []\n",
    "    raw_ocr = word_ocr[0]\n",
    "    for i in raw_ocr[\"Words\"]:\n",
    "        text_list.append(i[\"text\"])\n",
    "    return text_list\n",
    "\n",
    "def remember_write(datapoint):\n",
    "    resp_dict = {}\n",
    "    url = f\"{v2_url}/write\"\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            url=url, data=json.dumps(datapoint), headers=headers\n",
    "        )\n",
    "        resp_dict = resp.json()\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "    return resp_dict\n",
    "\n",
    "\n",
    "def create_datapoint(Type, Fields, TransactionId, Attributes=None):\n",
    "    datapoint = {\n",
    "        \"Type\": Type,\n",
    "        \"Fields\": Fields,\n",
    "        \"TransactionId\": TransactionId,\n",
    "    }\n",
    "    if Attributes != None:\n",
    "        datapoint[\"Attributes\"] = Attributes\n",
    "    return remember_write(datapoint)\n",
    "\n",
    "\n",
    "def remember_memorize(data, rid, datapoint, metadata={}):\n",
    "    url = f\"{v1_url}/memoorize\"\n",
    "    try:\n",
    "        metadata.update({\n",
    "            \"_remember_id\": rid,\n",
    "            \"_datapoint\": datapoint\n",
    "        })\n",
    "        payload = {\n",
    "            \"data\": data,\n",
    "            \"metadata\": metadata \n",
    "        }\n",
    "        resp = requests.post(\n",
    "                url=url, data=json.dumps(payload), headers=headers)\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "    return resp\n",
    "    \n",
    "\n",
    "def aggregate_formatted_entities(docid):\n",
    "    temp_dict = {}\n",
    "    try:\n",
    "        recall_txn = rememberv2_read(docid)[0]\n",
    "        txnid = recall_txn[\"TransactionId\"]\n",
    "        file_pages = recall_txn[\"Pages\"]\n",
    "        start = file_pages[0]\n",
    "        doc_pages = list(range(1, len(file_pages)+1))\n",
    "        page_ocrs_ids = {x['ParentIndex']:x['ObjectId'] for x in rememberv2_query({'PageOcr::TransactionId': txnid}, {'ParentIndex': file_pages})}\n",
    "        results = {}\n",
    "        formatted_doc = {}\n",
    "        for page in sorted(page_ocrs_ids.keys()):\n",
    "            try:\n",
    "                print(\"\\n==================WORDS ARE HERE========================\")\n",
    "                words_ocr = rememberv2_query({'Parent': page_ocrs_ids[page]}) \n",
    "                #test_this.append(words_ocr[0]) \n",
    "                lol.append(words_ocr)\n",
    "                temp_cleaned = make_text_blob(words_ocr[0])\n",
    "                \n",
    "                #test_this.append(temp_dict.update({\"rid\" : docid, \"words\" : temp_cleaned }))\n",
    "                print(\"\\n==================END========================\")\n",
    "                tagged = tagger.handler({'body': json.dumps(words_ocr[0])}, {})\n",
    "                formatted = entity_formatter.format_entities(json.loads(tagged['body'])['entities'], page-start+1)['body']\n",
    "                results[page] = formatted\n",
    "                create_datapoint(\"PageTaggedEntitiesExp\", {\"Entities\": formatted, \"FilePageIndex\": page, \"ExpId\": exp_id}, txnid ,{\"PageTaggedEntitiesExp::DocumentId\": docid})\n",
    "                for key in formatted.keys():\n",
    "                    if key in formatted_doc:\n",
    "                        formatted_doc[key] = formatted_doc[key] + formatted[key]\n",
    "                    else:\n",
    "                        formatted_doc[key] = formatted[key]\n",
    "            except:\n",
    "                print(traceback.format_exc())\n",
    "                pass\n",
    "        return formatted_doc\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_bucket_key(path):\n",
    "    bucket = path.split('/')[2]\n",
    "    key = path.replace(f'S3://{bucket}/', '')\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "def get_object(path, s3):\n",
    "    bucket, key = get_bucket_key(path)\n",
    "    res = s3.get_object(\n",
    "        Bucket=bucket,\n",
    "        Key=key\n",
    "    )['Body'].read().decode('utf-8')\n",
    "    return res\n",
    "\n",
    "\n",
    "def put_object(path, s3, data):\n",
    "    bucket, key = get_bucket_key(path)\n",
    "    s3.put_object(\n",
    "        Bucket=bucket,\n",
    "        Body=json.dumps(data),\n",
    "        Key=key\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "def memorize_results_update_inplace(docid):\n",
    "    formatted_doc = aggregate_formatted_entities(docid)\n",
    "    current_path = remember_recall(docid, '_aggregated_formatted_entities_path')\n",
    "    new_path = current_path.replace(\"FormattedEntities\", f\"FormattedEntities{exp_id}\")\n",
    "    put_object(new_path, s3, formatted_doc)\n",
    "    return new_path\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1003_rid_new.csv\",names=[\"rid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================WORDS ARE HERE========================\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-25-097064e5591e>\", line 104, in aggregate_formatted_entities\n",
      "    temp_cleaned = make_text_blob(words_ocr[0])\n",
      "  File \"<ipython-input-25-097064e5591e>\", line 40, in make_text_blob\n",
      "    raw_ocr = word_ocr[0]\n",
      "KeyError: 0\n",
      "\n",
      "\n",
      "==================WORDS ARE HERE========================\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-25-097064e5591e>\", line 104, in aggregate_formatted_entities\n",
      "    temp_cleaned = make_text_blob(words_ocr[0])\n",
      "  File \"<ipython-input-25-097064e5591e>\", line 40, in make_text_blob\n",
      "    raw_ocr = word_ocr[0]\n",
      "KeyError: 0\n",
      "\n",
      "\n",
      "==================WORDS ARE HERE========================\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-25-097064e5591e>\", line 104, in aggregate_formatted_entities\n",
      "    temp_cleaned = make_text_blob(words_ocr[0])\n",
      "  File \"<ipython-input-25-097064e5591e>\", line 40, in make_text_blob\n",
      "    raw_ocr = word_ocr[0]\n",
      "KeyError: 0\n",
      "\n",
      "\n",
      "==================WORDS ARE HERE========================\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-25-097064e5591e>\", line 104, in aggregate_formatted_entities\n",
      "    temp_cleaned = make_text_blob(words_ocr[0])\n",
      "  File \"<ipython-input-25-097064e5591e>\", line 40, in make_text_blob\n",
      "    raw_ocr = word_ocr[0]\n",
      "KeyError: 0\n",
      "\n",
      "\n",
      "==================WORDS ARE HERE========================\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-25-097064e5591e>\", line 104, in aggregate_formatted_entities\n",
      "    temp_cleaned = make_text_blob(words_ocr[0])\n",
      "  File \"<ipython-input-25-097064e5591e>\", line 40, in make_text_blob\n",
      "    raw_ocr = word_ocr[0]\n",
      "KeyError: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "temp[f\"{exp_id}_path\"] = temp.apply(lambda row: memorize_results_update_inplace(row[\"rid\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_words_bro = test_this[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e8c15dcbb4b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_of_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_text_blob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-097064e5591e>\u001b[0m in \u001b[0;36mmake_text_blob\u001b[0;34m(word_ocr)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mtext_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mraw_ocr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_ocr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_ocr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Words\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mtext_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "list_of_words = make_text_blob(lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

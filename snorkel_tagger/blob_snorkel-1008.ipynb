{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import entity_formatter\n",
    "from entity_tagger import entity_tagger as tagger\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import traceback\n",
    "import snorkel\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import entity_formatter\n",
    "from entity_tagger import entity_tagger as tagger\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import traceback\n",
    "import snorkel\n",
    "import json\n",
    "\n",
    "ssm = boto3.client(\"ssm\")\n",
    "s3 = boto3.client(\"s3\")\n",
    "root_url = ssm.get_parameter(Name=f\"/account/root-url\")[\"Parameter\"][\"Value\"]\n",
    "apikey = ssm.get_parameter(Name=\"/account/internal-api-key\")[\"Parameter\"][\"Value\"]\n",
    "v1_url = f\"https://remember.{root_url}\"\n",
    "v2_url = f\"https://rememberv2.{root_url}/latest\"\n",
    "acc_owner = ssm.get_parameter(Name=\"/account/owner\")[\"Parameter\"][\"Value\"].upper()\n",
    "headers = {\"x-api-key\": apikey, \"Authorization\": apikey}\n",
    "\n",
    "def rememberv2_query(index={}, filters={}):\n",
    "    url = f\"{v2_url}/query\"\n",
    "    results = {}\n",
    "    try:\n",
    "        payload = {\n",
    "            \"Index\": index,\n",
    "            \"Filter\": filters\n",
    "        }\n",
    "        results = json.loads(requests.post(url=url, data=json.dumps(payload), headers=headers).text)[\"Results\"]\n",
    "    except:\n",
    "        print(traceback.format_exc())    \n",
    "    return results\n",
    "\n",
    "\n",
    "def rememberv2_read(objectid):\n",
    "    url = f\"{v2_url}/read\"\n",
    "    results = {}\n",
    "    try:\n",
    "        payload = {\n",
    "            \"ObjectId\": objectid,\n",
    "        }\n",
    "        results = json.loads(requests.post(url=url, data=json.dumps(payload), headers=headers).text)[\"Results\"]\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def remember_recall(rid, datapoint):\n",
    "    url = f\"{v1_url}/recall?_remember_id={rid}&_datapoint={datapoint}\"\n",
    "    res = {}\n",
    "    try:\n",
    "        res = json.loads(requests.get(url=url).text)[\"datapoints\"][0][\"data\"]\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "    return res\n",
    "    \n",
    "# def make_text_blob(word_ocr):\n",
    "#     text_list = []\n",
    "    \n",
    "#     for i in word_ocr[\"Words\"]:\n",
    "#         text_list.append(i[\"text\"])\n",
    "#     #print(\"\\n\\n\\nBefore Sending it off: \" , text_list)\n",
    "#     return text_list\n",
    "\n",
    "def remember_write(datapoint):\n",
    "    resp_dict = {}\n",
    "    url = f\"{v2_url}/write\"\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            url=url, data=json.dumps(datapoint), headers=headers\n",
    "        )\n",
    "        resp_dict = resp.json()\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "    return resp_dict\n",
    "\n",
    "\n",
    "def create_datapoint(Type, Fields, TransactionId, Attributes=None):\n",
    "    datapoint = {\n",
    "        \"Type\": Type,\n",
    "        \"Fields\": Fields,\n",
    "        \"TransactionId\": TransactionId,\n",
    "    }\n",
    "    if Attributes != None:\n",
    "        datapoint[\"Attributes\"] = Attributes\n",
    "    return remember_write(datapoint)\n",
    "\n",
    "\n",
    "def remember_memorize(data, rid, datapoint, metadata={}):\n",
    "    url = f\"{v1_url}/memoorize\"\n",
    "    try:\n",
    "        metadata.update({\n",
    "            \"_remember_id\": rid,\n",
    "            \"_datapoint\": datapoint\n",
    "        })\n",
    "        payload = {\n",
    "            \"data\": data,\n",
    "            \"metadata\": metadata \n",
    "        }\n",
    "        resp = requests.post(\n",
    "                url=url, data=json.dumps(payload), headers=headers)\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "    return resp\n",
    "def do_sner_tag(text):\n",
    "    text = text.replace(\"/\",\"-\")\n",
    "    text = text.replace(\"[]\",\"\")\n",
    "    tagged_list = sner_tagger.tag(word_tokenize(text))\n",
    "    return tagged_list\n",
    "def do_spacy_tag(text):\n",
    "    text = text.replace(\"/\",\"-\")\n",
    "    \n",
    "\n",
    "def aggregate_formatted_entities(docid):\n",
    "    temp_dict = {}\n",
    "    try:\n",
    "        recall_txn = rememberv2_read(docid)[0]\n",
    "        txnid = recall_txn[\"TransactionId\"]\n",
    "        file_pages = recall_txn[\"Pages\"]\n",
    "        start = file_pages[0]\n",
    "        doc_pages = list(range(1, len(file_pages)+1))\n",
    "        page_ocrs_ids = {x['ParentIndex']:x['ObjectId'] for x in rememberv2_query({'PageOcr::TransactionId': txnid}, {'ParentIndex': file_pages})}\n",
    "        results = {}\n",
    "        formatted_doc = {}\n",
    "        count = 0\n",
    "        print(\"pages-->\",sorted(page_ocrs_ids.keys()))\n",
    "        for page in sorted(page_ocrs_ids.keys()):\n",
    "            try:\n",
    "                \n",
    "                #print(\"Going on a count: \", count)\n",
    "                count = count+1\n",
    "                words_ocr = rememberv2_query({'Parent': page_ocrs_ids[page]})\n",
    "                parsed_words = tagger.parse_words(words_ocr[0]['Words'])\n",
    "                blob = tagger.make_blob(parsed_words)\n",
    "                #lol.append(parsed_words)\n",
    "                #print(\"page_number --> \", page)\n",
    "                call_make_df(parsed_words,blob,page,docid)\n",
    "#                 tagged = tagger.handler({'body': json.dumps(words_ocr[0])}, {})\n",
    "#                 find_untagged_words(parsed_words,tagged)\n",
    "#                 formatted = entity_formatter.format_entities(json.loads(tagged['body'])['entities'], page-start+1)['body']\n",
    "#                 results[page] = formatted\n",
    "#                 create_datapoint(\"PageTaggedEntitiesExp\", {\"Entities\": formatted, \"FilePageIndex\": page, \"ExpId\": exp_id}, txnid ,{\"PageTaggedEntitiesExp::DocumentId\": docid})\n",
    "#                 for key in formatted.keys():\n",
    "#                     if key in formatted_doc:\n",
    "#                         formatted_doc[key] = formatted_doc[key] + formatted[key]\n",
    "#                     else:\n",
    "#                         formatted_doc[key] = formatted[key]\n",
    "            except:\n",
    "                print(traceback.format_exc())\n",
    "                pass\n",
    "        return formatted_doc\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        pass\n",
    "def process_tagged_with_text(page):\n",
    "    # extract all named entities\n",
    "    tagged_entities = []\n",
    "    index_count = 0\n",
    "    entity_id = ''\n",
    "    \n",
    "    for term, tag in sentence:\n",
    "        if tag != 'O':\n",
    "            word = term\n",
    "            word_tag = tag\n",
    "            entity_id = uuid.uuid4()\n",
    "            make_entity = {'entity_id': entity_id.hex, 'text': word, 'entity_score': 0.9902280569076538 , 'entity_type': word_tag,'string_index': index_count }                    \n",
    "            index_count = len(term)+index_count+1\n",
    "            tagged_entities.append(make_entity)\n",
    "        else:\n",
    "            index_count = len(term)+index_count+1\n",
    "        \n",
    "    return tagged_entities\n",
    "\n",
    "def find_untagged_words(untagged,tagged):\n",
    "    temp_tagged.append(tagged)\n",
    "    temp_untagged.append(untagged)\n",
    "\n",
    "def get_bucket_key(path):\n",
    "    bucket = path.split('/')[2]\n",
    "    key = path.replace(f'S3://{bucket}/', '')\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "def get_object(path, s3):\n",
    "    bucket, key = get_bucket_key(path)\n",
    "    res = s3.get_object(\n",
    "        Bucket=bucket,\n",
    "        Key=key\n",
    "    )['Body'].read().decode('utf-8')\n",
    "    return res\n",
    "\n",
    "\n",
    "def put_object(path, s3, data):\n",
    "    bucket, key = get_bucket_key(path)\n",
    "    s3.put_object(\n",
    "        Bucket=bucket,\n",
    "        Body=json.dumps(data),\n",
    "        Key=key\n",
    "    )\n",
    "def get_tagged_words(tagged):\n",
    "    list_of_tagged_word_ids = []\n",
    "    for page in tagged:\n",
    "        rip_a_page = json.loads(page[\"body\"])\n",
    "        for entity in rip_a_page[\"entities\"]:\n",
    "            list_of_tagged_word_ids.append(entity[\"word_id\"])\n",
    "    return list_of_tagged_word_ids    \n",
    "    \n",
    "def get_untagged_words(untagged,list_of_tagged_word_ids):\n",
    "    list_of_untagged_word_ids = []\n",
    "    list_of_untagged_entities = []\n",
    "    for page in untagged:\n",
    "        for entity in page:\n",
    "            list_of_untagged_word_ids.append(entity[\"word_id\"])\n",
    "    l3 = [x for x in list_of_untagged_word_ids if x not in list_of_tagged_word_ids]\n",
    "    for word in l3:\n",
    "        for page in untagged:\n",
    "            for entity in page:\n",
    "                if entity[\"word_id\"] == word:\n",
    "                    list_of_untagged_entities.append(entity)\n",
    "    return list_of_untagged_entities,l3\n",
    "    \n",
    "def memorize_results_update_inplace(docid):\n",
    "    formatted_doc = aggregate_formatted_entities(docid)\n",
    "    current_path = remember_recall(docid, '_aggregated_formatted_entities_path')\n",
    "    new_path = current_path.replace(\"FormattedEntities\", f\"FormattedEntities{exp_id}\")\n",
    "    put_object(new_path, s3, formatted_doc)\n",
    "    return new_path\n",
    "        \n",
    "def call_make_df(parsed_words,blob,page_num,docid):\n",
    "    for i in parsed_words:\n",
    "        i[\"text_blob\"] = blob\n",
    "        i[\"page_ind\"] = f'page_{page_num+1}'\n",
    "        i[\"rid\"] = docid\n",
    "        i[\"page\"] = page_num+1\n",
    "        full_lits.append(i)\n",
    "        \n",
    "\n",
    "temp_tagged = []\n",
    "temp_untagged = []\n",
    "page_blobs = []\n",
    "full_list = []\n",
    "lol = []\n",
    "\n",
    "df = pd.read_csv(sys.argv[2],names=[\"rid\"])\n",
    "\n",
    "for i,j in temp.iterrows():\n",
    "    aws_json = aggregate_formatted_entities(j.rid)\n",
    "\n",
    "training_frame = pd.DataFrame(full_lits)\n",
    "training_frame.to_csv(f\"{}_dataset.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tagged_word_ids = get_tagged_words(temp_tagged)\n",
    "list_of_untagged_entities, list_of_untagged_word_ids = get_untagged_words(temp_untagged,list_of_tagged_word_ids)\n",
    "list_of_untagged_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untagged_word_ids = [ent[\"word_id\"] for ent in list_of_untagged_entities]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Tagging experimentation with other taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_snre_pages = []\n",
    "for blob_by_page in page_blobs:\n",
    "    temp_tag = [sner_tagger.tag(word_tokenize(blob_by_page))]\n",
    "    temp_result_array = process_tagged_with_text(temp_tag)\n",
    "    tagged_snre_pages.append(temp_result_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_sner_tagged_entities = tagger.zip_words_entities(temp_untagged[0],tagged_snre_pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparelist = [word[\"word_id\"] for word in zipped_sner_tagged_entities[\"entities\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_blobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities(example, show=False):\n",
    "    if show: print(example)\n",
    "    parsedEx = parser(example)\n",
    " \n",
    "    print(\"-------------- entities only ---------------\")\n",
    "    # if you just want the entities and nothing else, you can do access the parsed examples \"ents\" property like this:\n",
    "    ents = list(parsedEx.ents)\n",
    "    tags={}\n",
    "    for entity in ents:\n",
    "        #print(entity.label, entity.label_, ' '.join(t.orth_ for t in entity))\n",
    "        term=' '.join(t.orth_ for t in entity)\n",
    "        if ' '.join(term) not in tags:\n",
    "            tags[term]=[(entity.label, entity.label_)]\n",
    "        else:\n",
    "            tags[term].append((entity.label, entity.label_))\n",
    "    print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_check.street_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datefinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = datefinder.find_dates(\"If this is an application for joint credit, Borrower and Co-Borrower each agree that we intend to apg_ly for joint credit sign below: DocuSigned by: DocuSigned by —thu'tl M Jolunson. $020312  \")\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
